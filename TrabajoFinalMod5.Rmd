---
title: "TrabajoFinalModulo5"
author: "María José Guzmán"
date: "2024-01-15"
output:
  word_document: default
  pdf_document: default
  html_document: default
---


**Cargar paquetes**

```{r}
library(openxlsx)
library(stats)
library(cluster)
library("fpc")
library("factoextra")
library(NbClust)
library(htmltools)
```


## Cargar base

```{r}

data <- read.xlsx("C:/Users/maria/Dropbox/Acádemico/Análisis de datos/Módulo 5/Base/parte 2/BOL_BP_MAY_2017v2.xlsx")
nombres <- data$BANCOS
```

*Modelo jerárquico*


*Se escala la base para mantener a una sola escala todas las variables*


```{r}
base <- as.data.frame(scale(data[,-1]))
row.names(base) <- nombres
```

*Se especifica la distancia y el metodo de aglomeracion*

Donde para este caso se usa el método Euclidean con Ward,D mientras que para el otro se utiliza el método de Manhattan con el promedio


```{r}
cluster <- hclust(dist(base, method = "euclidean"),
                  method = "ward.D")


plot(cluster,hang = -0.01,cex=0.8)

```


```{r}
cluster2 <- hclust(dist(base, method = "manhattan"),
                  method = "average")


plot(cluster2,hang = -0.01,cex=0.8)
```

**Compactar en 2 gráficos**

```{r}
par(mfrow=c(1,2))
plot(cluster,hang = -0.01,cex=0.8);plot(cluster2,hang = -0.01,cex=0.8)
```
Realizando los gráficos con estas dos combinaciones de métodos y distancias se observa que la euclidea con el método de Ward.D es quien mejor realiza los cluster debido a que en el caso de promedios con Manhattan se observan que muchos bancos quedan agrupados juntos por lo que quiere decir que existen bancos muy similares y otros valores extremos que estan quedando solos.

Tambien se realiza utilizando la función hcut, en este caso nos permite hacer otra combinación donde usamos entonces la que ya habíamos seleccionado de euclidean con Ward.D y la comparamos usando en h.metric una distancia basada en la correlación de pearson con el promedio.


```{r}
res <- hcut(base, k = 4, stand = TRUE, 
            hc_metric = "euclidean",hc_method = "ward.D")
res
```


```{r}
fviz_dend(res, rect = TRUE, cex = 0.5,
          k_colors = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"))
```







```{r}
res2 <- hcut(base, k = 4, stand = TRUE, 
            hc_metric = "pearson",hc_method = "average")
res2
```


```{r}
fviz_dend(res2, rect = TRUE, cex = 0.5,
          k_colors = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"))
```


Al observar ambos gráficos se observa que realmente usando la correlacion como medida de distancia se agrupan mejor los bancos. 



*Módelo no jerárquico*

**Media de cada cluster**


```{r}
cnj <- kmeans(base,4)

```


```{r}
aggregate(base,
          by=list(cnj$cluster),
          FUN=mean)
```



```{r}
cnj$centers
```

**Visualizando el cluster**

```{r}
fviz_cluster(cnj,data=base)
```


**Clusters óptimos**

```{r}
clustersoptimos <- NbClust(base,
                           distance = "euclidean",
                           min.nc = 2,
                           max.nc = 10,
                           method="average",
                           index = "all")


```

**Evaluando la clusterizacion**

La silueta es una medida que indica como de bien estan agrupados o asignados lo cluster este comprendida entre -1 y 1 la idea es que esta cercano a 1. En el siguiente cuadro se puede observar que esta sobre 0.66 lo que indica que es cercano a uno para el cluster 1.

```{r}
cnj <- kmeans(base,2)
silueta <- silhouette(cnj$cluster,dist(base,method="euclidean"))
  
fviz_silhouette(silueta)

```












